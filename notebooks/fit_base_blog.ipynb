{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8395fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import openpyxl\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3440400",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d782eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_blog_df = pd.read_csv('../data/base_blog.csv')\n",
    "print(base_blog_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_api_key = os.getenv('CLAUDE_API_KEY')\n",
    "claude_client = Anthropic(\n",
    "    api_key=claude_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97265ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_blog_similar_df = pd.read_csv('../data/50_run1/base_blog_similar.csv')\n",
    "print(base_blog_similar_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7808decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_blog_dissimilar_df = pd.read_csv('../data/50_run1/base_blog_dissimilar.csv')\n",
    "print(base_blog_dissimilar_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142da7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "\n",
    "def expand_constraints_subsets(\n",
    "    df: pd.DataFrame,\n",
    "    subset_sizes: list = [7, 15, 23, 31, 39],\n",
    "    output_path: str = \"../data/trial_prompt2/constraints_bucket_blog.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Expands each row of the input DataFrame by creating copies with progressively\n",
    "    larger subsets of constraints (1–N) based on `subset_sizes`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing a 'Constraints' column formatted with a\n",
    "        leading \"Constraints:\" line followed by numbered items (1., 2., ...).\n",
    "    subset_sizes : list\n",
    "        Constraint counts to include for each generated subset.\n",
    "    output_path : str\n",
    "        Path for saving the expanded DataFrame as a CSV.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Expanded DataFrame with a new 'selected_constraints' column and a\n",
    "        'subset_size' column for clarity.\n",
    "    \"\"\"\n",
    "\n",
    "    if \"Constraint\" not in df.columns:\n",
    "        raise ValueError(\"Input DataFrame must contain a 'Constraint' column.\")\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "    expanded_rows = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        constraints_text = row[\"Constraint\"]\n",
    "\n",
    "        # --- Clean and split constraints properly ---\n",
    "        # Remove \"Constraints:\" prefix if present\n",
    "        constraints_text = re.sub(r\"^Constraints:\\s*\", \"\", constraints_text.strip(), flags=re.IGNORECASE)\n",
    "\n",
    "        # Split on numeric list markers (e.g., \"1.\", \"2.\", ...)\n",
    "        constraints_list = re.split(r'\\n\\s*\\d+\\.\\s*', constraints_text)\n",
    "        constraints_list = [c.strip() for c in constraints_list if c.strip()]\n",
    "\n",
    "        total_constraints = len(constraints_list)\n",
    "        instruction_num = row.get(\"Instruction Number\", idx + 1)\n",
    "        logging.info(f\"Instruction #{instruction_num}: Found {total_constraints} constraints\")\n",
    "\n",
    "        for size in subset_sizes:\n",
    "            subset = constraints_list[:min(size, total_constraints)]\n",
    "\n",
    "            # Clean existing numbering and re-number consistently\n",
    "            cleaned_subset = [re.sub(r'^\\d+\\.\\s*', '', c).strip() for c in subset]\n",
    "            selected_text = \"\\n\".join(f\"{i+1}. {cleaned_subset[i]}\" for i in range(len(cleaned_subset)))\n",
    "\n",
    "            new_row = row.copy()\n",
    "            new_row[\"selected_constraints\"] = selected_text\n",
    "            new_row[\"subset_size\"] = min(size, total_constraints)\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "\n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "    expanded_df.to_csv(output_path, index=False)\n",
    "    logging.info(f\"Expanded dataset with constraint subsets saved to {output_path}\")\n",
    "\n",
    "    return expanded_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03805c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_constraints_subsets(df=base_blog_similar_df, output_path=\"../data/50_run1/constraints_bucket_blog_similar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe7e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_constraints_subsets(df=base_blog_dissimilar_df, output_path=\"../data/50_run1/constraints_bucket_blog_dissimilar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = \"\"\"You are a blog writer.\n",
    "# Revise or rewrite the provided base blog so it satisfies all listed constraints.\n",
    "# Maintain realism, tone, and narrative flow. Integrate constraints naturally.\n",
    "# Generate three distinctly different internal drafts (in your reasoning only),\n",
    "# then produce one final cohesive blog that reads as a single-author piece.\n",
    "# Only output the final merged blog. \n",
    "# \"\"\"\n",
    "\n",
    "# user_input = \"\"\"\n",
    "# Base Blog - {}\n",
    "# Constraints - {}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b15fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert creative writer trained to satisfy complex multi-constraint objectives.\n",
    "Your goal is to rewrite or expand the given base blog so that it fulfills *all* listed constraints.\n",
    "\n",
    "However, each constraint must be addressed through *distinct, independent content*.\n",
    "You must not reuse the same sentence or paragraph to satisfy multiple constraints.\n",
    "Your challenge is to maintain coherence and natural flow while ensuring every constraint\n",
    "is uniquely expressed through its own textual evidence.\n",
    "\n",
    "Follow this structured reasoning and generation protocol:\n",
    "\n",
    "Step 1 — Interpret the Task:\n",
    "Read the base blog and the list of constraints carefully.\n",
    "Identify conceptual clusters among the constraints.\n",
    "Within each cluster, ensure that constraints are still treated as separate creative targets.\n",
    "\n",
    "Step 2 — Creative Planning:\n",
    "For each constraint:\n",
    "- Write one unique paragraph or at least one complete sentence addressing it explicitly.\n",
    "- Vary style, sentence rhythm, and imagery between constraints.\n",
    "- Avoid repeating phrases or reusing identical syntactic structures.\n",
    "- Use transitions that preserve narrative flow but separate ideas logically.\n",
    "\n",
    "Step 3 — Expansion and Integration:\n",
    "Once all constraints have been addressed separately,\n",
    "weave them into a coherent blog post that reads as single-author text.\n",
    "Use natural transitions, metaphors, and connective logic to link ideas smoothly,\n",
    "but do not merge or collapse paragraphs that serve different constraints.\n",
    "\n",
    "Step 4 — Verification:\n",
    "Before finalizing, verify that:\n",
    "- Each constraint corresponds to unique textual evidence.\n",
    "- No two constraints rely on identical or nearly identical wording.\n",
    "- The final text feels organic, creative, and contextually unified.\n",
    "\n",
    "Output only the final blog post.\n",
    "Do not include explanations, lists, or reasoning steps.\n",
    "\"\"\"\n",
    "user_input = \"\"\"\n",
    "Base Blog - {}\n",
    "Constraints - {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062aa72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to track API call cost. \n",
    "\n",
    "def log_usage(tokens):\n",
    "    # Get the current date and time\n",
    "    current_time = datetime.now().strftime(\"%m-%d-%Y %H:%M:%S\")\n",
    "\n",
    "    # Write the date-time and tokens used to the file\n",
    "    with open(\"api_usage.txt\", \"a\") as file:\n",
    "        file.write(f\"{current_time} : {tokens}\\n\")\n",
    "\n",
    "\n",
    "def total_usage():\n",
    "    total_tokens = 0\n",
    "    with open(\"api_usage.txt\", \"r\") as file:\n",
    "        for line in file:\n",
    "            # Split the line into date-time and tokens\n",
    "            parts = line.split(\" : \")\n",
    "            if len(parts) == 2:\n",
    "                _, tokens_str = parts\n",
    "                tokens = int(tokens_str)\n",
    "                total_tokens += tokens\n",
    "\n",
    "    cost = (total_tokens*0.0015)/1000\n",
    "    print(\"Total tokens used so far: \", total_tokens)\n",
    "    print(f\"Total cost so far: {cost}$\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1fe648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_fn(instruction, model=\"gpt-4.1-mini\", system_prompt=system_prompt, log=False):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": instruction},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    log_usage(tokens=response.usage.total_tokens)\n",
    "    if log:\n",
    "        print(\"Tokens used:\", response.usage.total_tokens)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ef9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from time import sleep\n",
    "\n",
    "def fit_blogs_to_constraints(\n",
    "    df: pd.DataFrame,\n",
    "    chat_fn,\n",
    "    system_prompt: str,\n",
    "    model: str = \"gpt-4.1-mini\",\n",
    "    output_path: str = \"../data/trial_prompt2/fitted_blogs_summarized.csv\",\n",
    "    retry_attempts: int = 3,\n",
    "    delay: float = 1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits each base blog in the DataFrame to its corresponding set of constraints\n",
    "    using an LLM (e.g., GPT-4.1-mini), and saves the rewritten blogs to a new CSV.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with 'base_blog' and 'selected_constraints' columns.\n",
    "    chat_fn : callable\n",
    "        Function that performs the chat call, e.g. chat_fn(prompt, model, system_prompt, log=True).\n",
    "    system_prompt : str\n",
    "        The editing role prompt guiding the model's rewrite behavior.\n",
    "    model : str\n",
    "        Model identifier, default 'gpt-4.1-mini'.\n",
    "    output_path : str\n",
    "        File path where the updated CSV (with fitted blogs) will be saved.\n",
    "    retry_attempts : int\n",
    "        Number of retries per failed generation.\n",
    "    delay : float\n",
    "        Delay in seconds between retries.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The updated DataFrame containing a new 'fitted_blog' column.\n",
    "    \"\"\"\n",
    "\n",
    "    required_cols = {\"base_blog\", \"selected_constraints\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"Input DataFrame must contain {required_cols} columns.\")\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "    fitted_blogs = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        base_blog = row[\"base_blog\"]\n",
    "        selected_constraints = row[\"selected_constraints\"]\n",
    "        instruction_num = row.get(\"Instruction Number\", idx + 1)\n",
    "        logging.info(f\"Processing Instruction #{instruction_num} with subset size {row.get('subset_size', 'N/A')}\")\n",
    "\n",
    "        fitted_blog = \"\"\n",
    "\n",
    "        for attempt in range(1, retry_attempts + 1):\n",
    "            try:\n",
    "                user_prompt = (\n",
    "                    f\"Base Blog:\\n{base_blog}\\n\\n\"\n",
    "                    f\"Constraints to Satisfy:\\n{selected_constraints}\\n\\n\"\n",
    "                )\n",
    "\n",
    "                response = chat_fn(user_prompt, model=model, system_prompt=system_prompt, log=True)\n",
    "                fitted_blog = response.choices[0].message.content.strip()\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Attempt {attempt}/{retry_attempts} failed for Instruction #{instruction_num}: {e}\")\n",
    "                if attempt < retry_attempts:\n",
    "                    sleep(delay)\n",
    "                else:\n",
    "                    logging.error(f\"Failed to generate fitted blog for Instruction #{instruction_num} after {retry_attempts} attempts.\")\n",
    "\n",
    "        fitted_blogs.append(fitted_blog)\n",
    "\n",
    "    # Add fitted_blog column\n",
    "    df[\"fitted_blog\"] = fitted_blogs\n",
    "\n",
    "    # Save updated CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    logging.info(f\"Fitted blogs saved to {output_path}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb23d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_bucket_similar_df = pd.read_csv('../data/50_run1/constraints_bucket_blog_similar.csv')\n",
    "#print 1 row of constraints_bucket_df\n",
    "print(constraints_bucket_similar_df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44932d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_bucket_dissimilar_df = pd.read_csv('../data/50_run1/constraints_bucket_blog_dissimilar.csv')\n",
    "#print 1 row of constraints_bucket_df\n",
    "print(constraints_bucket_dissimilar_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d602ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(system_prompt)\n",
    "fit_blogs_to_constraints(df=constraints_bucket_similar_df, output_path=\"../data/50_run1/fit_blog_similar.csv\", chat_fn=chat_fn, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290be0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_blogs_to_constraints(df=constraints_bucket_dissimilar_df, output_path=\"../data/50_run1/fit_blog_dissimilar.csv\", chat_fn=chat_fn, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9fcf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ask GPT to summarize the fitted blogs\n",
    "system_prompt = \"\"\"Given the blog post, rewrite a summarized version of the blog that is approximately 25% of the original length. \n",
    "Preserve the logical flow, tone, and all key insights. \n",
    "Ensure no critical data or arguments are omitted, but remove redundancies, filler sentences, and excessive examples.\n",
    "\"\"\"\n",
    "\n",
    "def summarize_blog(df: pd.DataFrame, chat_fn, system_prompt: str, model: str = \"gpt-4.1-mini\", output_path: str = \"../data/trial_prompt2/25_percent_summarized_blogs.csv\", retry_attempts: int = 3, delay: float = 1.0):\n",
    "    \"\"\"\n",
    "    Summarizes each blog post in `df` using an LLM chat function\n",
    "    and saves the results as a CSV.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame with 'blog' column.\n",
    "    chat_fn (callable): Function for LLM chat, e.g. chat_fn(prompt, model, system_prompt, log=True).\n",
    "    system_prompt (str): System-level prompt defining model behavior.\n",
    "    model (str): LLM model identifier, default 'gpt-4.1-mini'.\n",
    "    output_path (str): Path for saving the new CSV.\n",
    "    \"\"\"\n",
    "    if 'fitted_blog' not in df.columns:\n",
    "        raise ValueError(\"Input DataFrame must contain a 'fitted_blog' column.\")\n",
    "    \n",
    "    summarized_blogs = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        fitted_blog = row['fitted_blog']\n",
    "        for attempt in range(1, retry_attempts + 1):\n",
    "            try:\n",
    "                response = chat_fn(fitted_blog, model=model, system_prompt=system_prompt, log=True)\n",
    "                summarized_blog = response.choices[0].message.content.strip()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Attempt {attempt}/{retry_attempts} failed for instruction {instruction_num}: {e}\")\n",
    "        \n",
    "        summarized_blogs.append(summarized_blog)\n",
    "\n",
    "    df['fitted_summarized_blog'] = summarized_blogs\n",
    "    \n",
    "    df.to_csv(output_path, index=False)\n",
    "    logging.info(f\"Summarized blogs saved to {output_path}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c21fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_bucket_similar_df = pd.read_csv(\"../data/50_run1/fit_blog_similar.csv\")\n",
    "summarize_blog(df=constraints_bucket_similar_df, chat_fn=chat_fn, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e47b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/50_run1/fit_blog_similar.csv\")\n",
    "\n",
    "# Rename columns to match evaluation script\n",
    "df = df.rename(columns={\n",
    "    \"fitted_summarized_blog\": \"FinalGeneratedStory\",\n",
    "    \"selected_constraints\": \"SelectedConstraints\",\n",
    "    \"subset_size\": \"Number_of_Constraints\"\n",
    "})\n",
    "\n",
    "# Save to a new file for evaluation\n",
    "df.to_csv(\"../data/50_run1/for_evaluation_fit_blog_similar.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
