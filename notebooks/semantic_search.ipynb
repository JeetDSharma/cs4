{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1a9424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeetsharma/Documents/UMass/independent-study/dolma/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle, faiss, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "with open(\"outputs/ROC-spring-embeddings-all-mpnet-base-v2-size-100000.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "corpus_sentences = data[\"sentences\"]\n",
    "corpus_embeddings = data[\"embeddings\"].astype(\"float32\")\n",
    "\n",
    "index = faiss.read_index(\"outputs/blog_index.faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd4a4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, top_k=5):\n",
    "    query_vec = model.encode(query).astype(\"float32\")\n",
    "    query_vec /= np.linalg.norm(query_vec, keepdims=True)\n",
    "    query_vec = np.expand_dims(query_vec, axis=0)\n",
    "\n",
    "    distances, corpus_ids = index.search(query_vec, top_k)\n",
    "    for score, idx in zip(distances[0], corpus_ids[0]):\n",
    "        print(f\"{score:.3f} :: {corpus_sentences[idx][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755a2ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.629 :: Bringing a baby into this world is supposed to be one of the happiest days in parents' lives. However, when a child is injured due to medical negligence during delivery, the once joyous day can sudden...\n",
      "0.620 :: A. Recently, an analysis of 2012 medical malpractice payouts was made public. This analysis showed that there were 12,142 medical malpractice payouts last year, with five states making up the majority...\n",
      "0.617 :: One. In some routine medical and surgical procedures, the risks are low enough for most patients that it can be a shock when something goes seriously wrong. Tonsillectomy, for instance, is a very comm...\n",
      "0.690 :: I’m Amanda. I live and work as a Graphic Designer in Alberta, Canada. I’ve decided to write this blog to catalog my experiences and because I like to write and pretend people are listening. I spend mo...\n",
      "0.687 :: I absolutely LOVE cooking and love eating! My attitude toward food and healthy living is the “everything in moderation” approach. Eating healthily is a big part of my life but i also don’t deprive mys...\n",
      "0.644 :: About me This Post was written by Gemma from Dressing for Dinner 1. Where are you based? Edinburgh, Scotland although originally born and raised in Sussex. 2. How long have you been blogging? Only sin...\n"
     ]
    }
   ],
   "source": [
    "search(\"a family sues after a medical mistake\", top_k=3)\n",
    "search(\"blog about cooking and humility\", top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7028395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 25 pairs.\n",
      "Saved 25 pairs to outputs/similar_blog_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Load model data and FAISS index\n",
    "embedding_cache_path = \"outputs/ROC-spring-embeddings-all-mpnet-base-v2-size-100000.pkl\"\n",
    "index_path = \"outputs/blog_index.faiss\"\n",
    "\n",
    "with open(embedding_cache_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "corpus_sentences = data[\"sentences\"]\n",
    "corpus_embeddings = data[\"embeddings\"].astype(\"float32\")\n",
    "corpus_embeddings /= np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "index = faiss.read_index(index_path)\n",
    "\n",
    "# Parameters\n",
    "top_k = 10\n",
    "similarity_min = 0.70\n",
    "similarity_max = 0.80\n",
    "pairs = []\n",
    "used_ids = set()\n",
    "\n",
    "for i in range(len(corpus_sentences)):\n",
    "    if len(pairs) >= 25:\n",
    "        break\n",
    "    if i in used_ids:\n",
    "        continue\n",
    "\n",
    "    query_vec = np.expand_dims(corpus_embeddings[i], axis=0)\n",
    "    distances, corpus_ids = index.search(query_vec, top_k)\n",
    "\n",
    "    for score, j in zip(distances[0][1:], corpus_ids[0][1:]):  # skip self\n",
    "        if j == i or j in used_ids:\n",
    "            continue\n",
    "        if similarity_min <= score <= similarity_max:\n",
    "            pairs.append({\n",
    "                \"blog_1_id\": i,\n",
    "                \"blog_1_text\": corpus_sentences[i][:500],\n",
    "                \"blog_2_id\": j,\n",
    "                \"blog_2_text\": corpus_sentences[j][:500],\n",
    "                \"similarity\": round(float(score), 3)\n",
    "            })\n",
    "            used_ids.add(i)\n",
    "            used_ids.add(j)\n",
    "            break\n",
    "\n",
    "print(f\"Collected {len(pairs)} pairs.\")\n",
    "\n",
    "# Save to CSV\n",
    "output_file = \"outputs/similar_blog_pairs.csv\"\n",
    "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=pairs[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(pairs)\n",
    "\n",
    "print(f\"Saved {len(pairs)} pairs to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c01da67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 25 pairs.\n",
      "Saved 25 pairs to outputs/dissimilar_blog_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Load model data and FAISS index\n",
    "embedding_cache_path = \"outputs/ROC-spring-embeddings-all-mpnet-base-v2-size-100000.pkl\"\n",
    "index_path = \"outputs/blog_index.faiss\"\n",
    "\n",
    "with open(embedding_cache_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "corpus_sentences = data[\"sentences\"]\n",
    "corpus_embeddings = data[\"embeddings\"].astype(\"float32\")\n",
    "corpus_embeddings /= np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "index = faiss.read_index(index_path)\n",
    "\n",
    "# Parameters\n",
    "top_k = 10\n",
    "similarity_min = 0.30\n",
    "similarity_max = 0.40\n",
    "pairs = []\n",
    "used_ids = set()\n",
    "\n",
    "for i in range(len(corpus_sentences)):\n",
    "    if len(pairs) >= 25:\n",
    "        break\n",
    "    if i in used_ids:\n",
    "        continue\n",
    "\n",
    "    query_vec = np.expand_dims(corpus_embeddings[i], axis=0)\n",
    "    distances, corpus_ids = index.search(query_vec, top_k)\n",
    "\n",
    "    for score, j in zip(distances[0][1:], corpus_ids[0][1:]):  # skip self\n",
    "        if j == i or j in used_ids:\n",
    "            continue\n",
    "        if similarity_min <= score <= similarity_max:\n",
    "            pairs.append({\n",
    "                \"blog_1_id\": i,\n",
    "                \"blog_1_text\": corpus_sentences[i][:500],\n",
    "                \"blog_2_id\": j,\n",
    "                \"blog_2_text\": corpus_sentences[j][:500],\n",
    "                \"similarity\": round(float(score), 3)\n",
    "            })\n",
    "            used_ids.add(i)\n",
    "            used_ids.add(j)\n",
    "            break\n",
    "\n",
    "print(f\"Collected {len(pairs)} pairs.\")\n",
    "\n",
    "# Save to CSV\n",
    "output_file = \"outputs/dissimilar_blog_pairs.csv\"\n",
    "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=pairs[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(pairs)\n",
    "\n",
    "print(f\"Saved {len(pairs)} pairs to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
